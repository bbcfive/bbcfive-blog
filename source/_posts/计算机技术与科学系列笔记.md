---
title: 计算机技术与科学系列笔记
date: 2018-11-30 08:31:10
tags: computer science
categories: Crash Course
---

## 目录：

1.《计算机的早期历史》
2.《电子计算机》
3.《布尔逻辑和逻辑门》
4.《二进制》
5.《算数逻辑单元》
6.《寄存器和内存》
7.《中央处理器CPU》
8.《指令和程序》
9.《高级CPU设计》
10.《早期的编程方式》
11.《编程语言发展史》
12.《编程原理-语句和函数》
13.《算法入门》
14.《数据结构》
15.《阿兰图灵》
16.《软件工程》
17.《集成电路与摩尔定律》
18.《操作系统》
20.《文件系统》
21.《压缩》
22.《命令行界面》
23.《屏幕&2D图形显示》
24.《冷战和消费主义》
25.《个人计算机革命》
26.《用户图形界面》
27.《3D图形》
28.《计算机网络》
29.《互联网》
30.《万维网》
31.《计算机安全》
32.《黑客与攻击》
33.《加密》
34.《机器学习与人工智能》
35.《计算机视觉》
36.《自然语言处理》
37.《机器人》

 

## 1.《计算机的早期历史》
设备进化过程：算盘-> 步进计算器->差分机 ->分析机 ->打孔卡片制表机
最早的计算设备是算盘，渐渐地善于计算的人被叫做computer；后来computer从指代职业变成指代机器。
有名的步进计算器，是第一个可以做加减乘除的机器（乘除可以用加减表示）。
炮弹为了精准，要计算弹道，二战是查表来做（类似我们的乘法口诀表），但每次更改设计就需要做一张新表。
Charles Babbage提出了“差分机”，在构造差分机期间，想出了分析机，分析机是通用计算机。
Lovelace给分析机写了假想程序（她预测在未来会有一门专门的语言可以进行专业的分析），因此成为了第一位程序员。
人口普查10年一次，Herman Hollerith的打孔卡片制表机大大提升了效率。

## 2.《电子计算机》
设备的演变：继电器->真空管 ->晶体管
20世纪的发展要求更强的计算能力，柜子大小的计算机发展到房间大小。
哈佛Mark 1号，IBM 1944年做出来。
继电器1秒最多50次开关，并且极易损坏，差不多半天损坏一个；并且容易招惹小虫子（bug）。
1904年，热电子管出现，这是第一个真空管。它通过控制电子来工作，改进功能后表现的和继电器功能一样，但速度更快。
“巨人1号”计算机在英国 布莱切利园 首次大规模使用真空管，但编程麻烦，还要配置。
1946年，宾夕法尼亚大学的ENIAC是第一个通用可编程计算机。
1947年，为了降低成本和提高稳定性与速度，贝尔实验室做出了晶体管，晶体管有诸多好处，IBM很快全面转向晶体管。
晶体管就像继电器或真空管，它是一个开关，可以由控制路线来控制开或关。晶体管有两个电极，电极之间有一种材料隔开它们，这种材料有时候导电，有时不导电，这叫
半导体。控制线连到一个“门”电极，通过改变一个门的电荷，我们可以控制半导体材料的导电性。来允许或不允许电流流动。晶体管是固态的，体积比前两者小很多，使得计算机变得小而便宜。
硅谷有很多硅，很多晶体管和半导体都是在这里做的。
肖克利半导体->仙童半导体 ->英特尔

## 3.《布尔逻辑和逻辑门》
当事件只有true和false两种状态时，可以考虑用0和1表示假或真，即用二进制来表示。
布尔逻辑具有三个基本操作：NOT/AND/OR
XOR代表异或，它们用于设计电路，从而形成处理器的底层逻辑。

## 4.《二进制》
用0和1来表示，基数为2，逢二进一。
一位1或0代表1bit（位）
8bit=1byte，1字节（byte）最多有2^8=256个值
1kb=1024b=1024*8bit
计算机分为32bit和64bit，64位能储存2^64次方的数值，相当大。计算机通常用第一位代表正负，0为-，1为+。

## 5.《算数逻辑单元》
算数逻辑单元，简称ALU，最早一款是英特尔做的74181；
ALU有2个单元，1个算数单元和1个逻辑单元。
算术单元：半加器（处理1个bit，2个输入）、全加器（处理1个bit，3个输入）、8bit加法（1个半加器，7个全加器）
逻辑单元：检测数字是否为0的电路（一堆OR门最后加个NOT门）、ALU被抽象成一个V符号使用（接受2位输入，输入8位结果）、Flags标志（判断overflow、zero、negative等等）

## 6.《寄存器和内存》
锁存器（Gated Latch）每次存一位；寄存器（Register）每次存8位。16*16的矩阵存256位。
数据选择器/多路复用器（Multiplexer）解码8位地址，定位到单个锁存器中。前四位代表行，后四位代表列。
一条1980年代的1M大小的内存由8个模块组成，每个模块有32个小方块，每个小方块有4个小块，每小块是128位*64位。

## 7.《中央处理器CPU》
CPU组成：RAM+寄存器+ALU
CPU如何执行命令：取指令->解释 ->执行
首先，有一个指令表，表的属性有：Instruction/Description/4-Bit Opcode/Address or Registers.
取指令阶段（FETCH PHASE）：RAM中存着指令；将指令地址寄存器连到RAM，从00000000开始取值，每周期+1，对应到RAM里取址；所取相应地址的指令值存储在指令寄存器中。
解码阶段（DECODE PHASE）：指令寄存器中的值，假设为‘00101110’，前4位0010去指令表查对应操作指令。后4位1110是RAM的地址，需转为10进制。接下来，指令由控制单元（ALU）进行解码。
执行阶段（EXECUTE PHASE）：用检查指令的电路，打开RAM的“允许读取线”，把地址传过去，RAM拿到值后存储进Data里，最后放入指令指定的寄存器中。此时把指定地址寄存器值+1，执行阶段（此周期）完成。

## 8.《指令和程序》
随机存取存储器（random access memory，RAM）：又称作“随机存储器”，是与CPU直接交换数据的内部存储器，也叫主存(内存)。
CPU是硬件，但它配合RAM执行一系列的指令。指令和数据都是存在同一个内存里的（eg：00101110，即Load_A 14）。但Halt指令没有地址数据。
软件能做到硬件做不到的事。ALU没有除法功能，是除法程序馥赋予了此功能。没有硬件，软件跑不了；没有软件，硬件能力有限。在硬件基础上开出软件之花，软件最美的，在于一层层抽象。
CPU的指令和数据地址都在一起，后4位代表地址；但4位二进制只有16个地址，则只能操作16个地址。现代CPU有两种策略：
一、用更多的位来代表指令，比如32位或64位，这叫“指令长度”；
二、实行“可变指令长度”，比如，有的CPU用8位长度操作码（这叫“立即值”），有的用4位。这样设计指令可以是任意长度，但会让读取阶段（fetch cycle）复杂一点点。

## 9.《高级CPU设计》
早期为提升CPU速度，方法是加快晶体管切换速度。
CPU有专门的除法电路+其他电路来做复杂操作，用来处理游戏、视频解码等工作。
现代提升CPU性能的方法：
一、缓存
为了提高CPU的存取速度，在CPU内部装了一块缓存，通常只有几M左右大小，用来从RAM中整块整块提取数据，并在CPU中处理。
缓存里每块空间有一个特殊的标记，叫“脏位（dirty bit）”。当缓存中的内容被CPU改写时，RAM需要同步更新。因此，同步一般发生在缓存满了而CPU又要缓存时，在清理缓存腾出空间之前，会先检查脏位。如果是脏的，在加载新内容之前，会把数据写回RAM。
二、指令流水线
取指令->解码 ->执行不断重复，但每阶段用的是CPU的不同部分，因此可以并行处理。执行一个指令，同时解码下一个指令，同时读取下下个指令。但问题是，不同指令间存在依赖关系。因此，流水线处理器要先弄清楚指令间的依赖关系，必要时停止流水线，避免出现问题。高端CPU，会进一步动态排序有依赖关系的指令，最小化流水线停工时间。这叫“乱序执行。”第2个问题是，条件跳转（jump功能），这些指令会改变程序的执行流。高端CPU会把jump看成岔路口，然后猜哪条路的可能性大一些，然后提前把指令放进流水线，这叫“推测执行”。如果猜错了，CPU再清空流水线。为了提升正确率，CPU的开发商开发了复杂的算法，叫“分支预测”，来猜测哪条分支更有可能。
三、多核处理器
同时运行多个指令流，即在一个CPU芯片里，有多个独立处理单元（core1、core2...）。它们共享资源，比如缓存，使得多核可以合作运算；多核不够时，可以用多个CPU。

## 10.《早期的编程方式》
打孔纸卡->插线板 ->面板拨开关
插线板也即控制面板，上面插很多线。运行不同的程序要重新接线。

## 11.《编程语言发展史》
编程：二进制->助记符(汇编器) ->A-0(编译器) ->FORTRAIN
新语言：
1960年代：ALGOL, LISP, BASIC
1970年代：Pascal, C, Smalltalk
1980年代：C++, Objective-C, Perl
1990年代：Python, Ruby, Java
2000年代：Swift, Go

## 12.《编程原理-语句和函数》
变量、赋值语句
判断、条件、循环、函数、算法

## 13.《算法入门》
算法在日常生活中被广泛应用，现已是计算机科学的重要组成部分，它影响着我们的网络世界。
应用示例：
排序算法：邮件列表按时间更新排序；网页按关联度大小排序...现有排序算法达上百种，选择排序、冒泡排序、归并排序...
图算法：高德地图算最短距离（Dijkstra）。

## 14.《数据结构》
数组、字符串、矩阵、结构体、指针、节点、链表、队列、栈、树、二叉树、图...红黑树、堆

## 15.《阿兰图灵》
图灵尝试解决由德国科学家提出的可判定性问题，即有没有一个机器，你问一个问题，它能回答是或不是。为此，图灵创造出了图灵机模型。
阿隆佐邱奇发明了lambda算子来尝试解决这一问题。
图灵用图灵机很好的解决了“停机问题”，并认为这是一个悖论。同时揭示，计算机并不能完美解决所有问题。
二战期间，图灵在布莱切利园破解了德国英格玛加密机。
图灵测试：图灵提出，如果计算机能欺骗人类相信它是人类，才算是只能。这被认为是智能测试的基础，图灵因此被认为是人工智能之父。

## 16.《软件工程》
对象：包括对象、函数和变量。
面向对象编程：需要什么对象，仅负责此对象的功能实现，而不用管整个项目的进程。
应用程序接口（Application Programming Interface，API）：方便别的项目组高效调用我们的成果，需要为我们的程序创建一个接口，便于其他团队使用。
集成开发环境（Intergrated Development Environment，IDE）：涵盖了很多功能，包括开发、调试、测试etc
文档和注释：readme、comment，方便别人理解你的代码
版本控制（Version Control）：github一样，便于多人协作
质量控制（Quality Assurance testing，QA）：测试、验证程序的可行性
软件在正式发布前有beta版本，供少部分大众公开测试使用，已尽可能地发现问题；在此之前还有一个alpha版本，非常粗糙，仅在公司内部内测。

## 17.《集成电路与摩尔定律》
这章难度有点大，重点是晶圆的制作流程：光刻。还需要看更多的资料，先记录一下主要知识点：
分立元件（Discrete components）
数字暴政（Tyranny of Numbers）：如果想加强电脑性能，就需要更多部件，这导致更多线路，更复杂，所以很难做。
光刻（photolithography）
晶圆（wafer）
光刻胶（photoresist）
光掩模（photomask）
掺杂（doping）
摩尔定律（Moore's law）
摩尔定律的总结：进一步小型化会遇到2个问题：1.光的波长不足以制作更精细的设计；2.量子隧穿效应

## 18.《操作系统》
操作系统连接硬件和软件，主要包括多任务处理、内存的动态按分配（虚拟内存和保护内存）、以及I/O short等等。

## 20.《文件系统》
计算机的文件格式有许多，比如文本格式（txt）、音频格式（wav）和图片格式（bmp）等等。这些文件格式包括其他许多文件格式在计算机底层全都是以二进制的字符串（010101）来存储的。
1.文本格式：二进制-> 十进制-> ASCII码
2.音频格式：wav声波产生的振幅被转化为数字，再转为二进制
3.图片格式： 元数据包括长宽、数据维度和颜色深度等一系列参数所规定
8bit（位）=1byte（字节），因为8个二进制码对应成1个十进制码对应成一个ascii码

## 21.《压缩》
压缩是为了降低文件占用空间大小，加快传输速度。
无损压缩：没有损失任何数据。
游程编码（run-length coding）：记录相同元素个数，删掉重复数据。
霍夫曼树：也是字典编码。利用相同元素值合并，生成树状结构，再用0、1标注，做成字典。
有损压缩：利用人类的感知不完美而进行压缩。
感知编码：人类在视觉和听觉上都是不够完美的，因此，某些文件基于此的压缩不会给人造成视觉、听觉上的信息传递困扰。
时间冗余（temporal redundancy）：不同帧之间的差异似乎不大，有时只是某帧的小部分内容在变。对于每帧间相似的地方，只需给视频增加补丁，而不用记录每帧的所有元素，即可减少冗余部分，从而降低文件大小。

## 22.《命令行界面》
计算机早期的人机交互形式是：同时输入程序和数据（用纸卡/纸带），运行开始直到结束，中间没有人类操作，原因是计算机很贵，不能等人类慢慢输入，执行完结果打印到纸上。
到1950年代，计算机足够便宜和快，人机交互式操作变得可行。为了让人类输入到计算机，改造之前就有的打印机，变成电传打字机。
到1970年代末，屏幕成本足够低，屏幕代替电传打印机成为标配。

## 23.《屏幕&2D图形显示》
在早期，计算机的键盘和显示器是分开的，因为屏幕太贵且内存很小，因此屏幕显示临时值。后来人们使用阴极射线管（cathode ray tube），即CRT来处理界面。原理是把电子发射到有磷光体涂层的屏幕上，当电子撞击涂层时，会发光几分之一秒。由于电子是带电粒子，路径可以用磁场控制。屏幕内用板子或线圈，把电子引导到想要的位置。
CRT有两种绘图方式：矢量扫描（Vector Scaning）和光栅扫描（Raster Scaning）。前者引导电子束描绘形状，将电子按照一定的引导路径被打到屏幕上形成图像；后者按固定路径，一行行从上到下，从左到右，不断重复，对屏幕进行逐行扫描，只在特定的点打开电子束，以此绘制图形。
最后，因为显示技术的发展，我们终于可以在屏幕上显示清晰的点，叫像素。
由于内存小，早期计算机不存大量像素值，而是存符号。为此，计算机需要额外的硬件来从内存中读取字符，转换成光栅图形，这样才能显示到屏幕上。这个硬件叫“字符生成器”，基本上算是第一代显卡。它内部有一小块只读存储器，简称ROM，存着每个字符的图形，叫“点阵图案”。
为了显示，“字符生成器”会访问内存中一块特殊的区域，这块区域专为图形保留，叫屏幕缓冲区。程序想显示文字时，修改这块区域里的值就行。

## 24.《冷战和消费主义》
冷战导致美国往计算机领域投入大量的资源。
1950年代消费者开始购买晶体管设备，收音机大卖。日本取得晶体管授权后，索尼做了晶体管收音机，为日本半导体行业崛起埋下了种子。
苏联1961年把宇航员加加林送上太空，导致美国提出登月。NASA预算大大增加，用集成电路来制作登月计算机。
集成电路的发展实际上是由军事应用大大推进的，美国造超级计算机进一步推进集成电路。
美国半导体行业一开始靠政府高利润合同活着，忽略消费者市场，1970年代冷战渐消，行业开始衰败，很多公司倒闭，英特尔转型处理器。
小结：政府和消费者推动了计算机的发展，早期靠政府资金，让技术发展到足够商用，然后消费者购买商用产品继续推动产品发展。

## 25.《个人计算机革命》
1970年初成本下降，个人计算机变得可行。
Altair 8800出现后，比尔盖茨和保罗艾伦写出Basic 解释器，使得用户可以用简单的basic语言来与计算机进行交互，而非复杂的源代码。
乔布斯提议卖组装好的计算机，Apple-I 诞生；
1977年出现3款开箱即用的计算机，此时IBM意识到个人计算机市场。
IBM PC 发布，采用开放架构系统，兼容的机器都叫 IBM Compatible（IBM 兼容）生态系统出现雪球效应：
因为用户多，软硬件开发人员更愿意花精力在这个平台。
因为软硬件多，用户也更乐意买“IBM 兼容”的计算机。
而苹果选封闭架构，一切都自己来，只有苹果在非“IBM 兼容”下保持了足够的市场份额。

## 26.《用户图形界面》
图形界面先驱：道格拉斯恩格尔巴特（Douglas Engelbart）因为提出并发明图形界面，以增强人类智能的愿景，获得了1997年的图灵奖。
1973年Xerox Alto（施乐奥托）计算机出现，1981年诞生施乐之星系统。
乔布斯去施乐参观后提出“所见即所得（WYSIWYG）”的交互理念。
1983年推出Apple Lisa失败，1984年推出Macintosh因用户图形界面而大获成功。
1995年Windows推出Microsoft Bob因浮夸的用户图形界面而失败；同年，推出Windows 95，提供用户图形界面，获得成功。

## 27.《3D图形》
线框渲染（Wireframe Rendering）：所有的点从3D转到2D之后，就可以用画2D线段的函数来链接这些点。
正交投影 Orthographic Projection：从物体不同的面等比例投影
透视投射 Perspective Projection：对物体进行近视距、远视距投影
网络 Mesh：由很多的多边形（polygons）组成。游戏设计者要平衡角色的真实性和多边形的数量。
扫描线渲染（Scanline Rendering）：填充图形的经典算法。对于一个多边形，先在其上铺一层像素网络，扫描线算法先读多边形的3个点，找出最大和最小Y值，只在这两点间工作。然后算法从上往下，一次处理一行，计算每一行和多边形相交的2个点，并填满2个相交点之间的像素，这是其工作原理。填充的速度叫fillrate（填充速率）。 这样的三角形可能会比较丑（周遭都是锯齿），当像素较小时，就不那么明显。
抗锯齿（Antialiasing）：减轻锯齿的方法，通过判断多边形切过像素的程度，来调整颜色；对于在多边形内部的像素，就直接涂颜色；对于多边形边缘划过的像素，颜色就浅一些。这种羽化的效果看起来更舒适。
遮挡（Occlusion）：在3D图形中，有一些多边形无法被看见。
画家算法（Painter's Algorithm）：用排序算法，从远到近排列，从远到近渲染。
深度缓冲（ Buffering）：记录场景中每个像素和摄像机的距离，在内存里存一个数字矩阵，首先，每个像素的距离被初始化为“无限大”，然后Z-buffering从列表力量第一个多边形开始处理，它和扫描线算法逻辑相同，但不是给像素填充颜色，而是把多边形的距离和X-buffering里的距离进行对比，它总是记录更低的值。A距离20，小于“无限大”，所以缓冲区记录20；然后计算下一个，以此类推。因为没对多边形排序，所以后处理的多边形并不总会覆盖前面的。当两个多变形距离相同时，哪一个画上面？这往往不可预测，因而导致出现了Z-fighting效果。
背面剔除（Back Face Culling）：游戏优化中，为节省处理时间，会忽略多边形背面。
表面法线（Surface Normal）：即平面面对的方向。
平面着色（Flat Shading）：根据每个多边形的表面法线来决定应给予的光线数量来着色，被称为平面着色，是最基本的照明手法。但这使得多边形的边界非常明显，看起来不光滑。因此发明了更多算法，高洛德着色（Gouraud shading）、冯氏着色（Phong shading）被用于更好地改善着色效果，使其看起来更加平滑。
纹理映射（Texture Mapping）：参照内存内的纹理图像，决定像素用什么颜色。因此，需要把多边形坐标和纹理坐标对应起来，算法依据需要填充的多边形坐标，从相应纹理区域内取平均颜色并填充多边形，重复这个过程，就可以获得纹理。
专门的图形处理器：图形处理单元（GPU, Graphics Processing Unit ）。GPU在显卡上，周围有专用的RAM。

## 28.《计算机网络》
局域网（Local Area Networks-LAN）：一小部分领域内互相连接的网络。
媒体访问控制地址（Media Access Control address-MAC）：每一个计算机都有一个自己的MAC address，用于数据传输时进行选择匹配验证。
载波侦听多路访问（Carrier Sense Multiple Access-CSMA）：多台电脑共享一个传输媒介。载体指运输数据的共享媒介，以太网的载体是铜线，WIFI的载体是传播无线电波的空气。许多计算机可以同时侦听载体，所以叫“侦听”和“多路访问”。而载体传输数据的速度叫“带宽”。
指数退避（Exponential Backoff）：是一种指数级增长等待时间的方法。如果一台计算机在传输数据期间检测到冲突，会等1秒+随机时间；然而，如果再次发生冲突，表示有网络拥塞。这次不等1秒，而是等2秒；如果再次发生冲突，等4s、8s、16s etc，直到传输成功。因为计算机的退避，冲突次数降低了，数据再次开始流动起来，网络变得顺畅。
冲突域（Collision Domain）：同一载体中设备的数量越少，网络速度越快。载体和其中的设备总称“冲突域”。以太网就创造2个冲突域，中间用一个switch隔开。交换数据时，MAC在哪边，就只动用那个冲突域的载体。从而降低congestion。
电路交换（Circuit Switching）：只有一条专有线路，把电路连接到正确的目的地。但不灵活且价格昂贵，因为总有闲置电路。好处是，如果有一条专属于自己的线路，就可以最大限度地随意使用，无需共享。
报文交换（Message Switching）：不仅只有一条专有线路，消息会经过好几个站点。好处是，可以用不同的路由，使通信更可靠更能容错。
阻塞控制：路由器会平衡与其他路由器之间的负载，以确保传输可以快速可靠，这叫“阻塞控制”。
分组交换（Packet Switching）：将数据拆分成多个小数据包，然后通过灵活的路由传递，非常高效且容错。好处是，去中心化，无政府，没有单点失败问题。

## 29.《互联网》
IP-互联网协议（Internet Protocol）：所有的数据包传输必须符合互联网协议（IP），它是一个很底层的协议。
PACKET=IP HEADER + DATA PAYLOAD
DATA PAYLOAD = UDP HEADER + DATA
数据包的头部只有目标地址，头部存“关于数据的数据”，也叫元数据（metadata）。
UDP-用户数据报协议（User Datagram Protocol）：
UDP的头部存有用的数据：信息之一是端口号，每个想访问网络的程序，都要向操作系统申请一个端口号（port）。
总结：IP负责把数据包送到正确的计算机；UDP负责把数据包送到正确的程序。
校验和（checksum）：也属于UDP头部，用于检查数据是否正确。UDP中，校验和以16位形式存储（即16个0或1）。超过16位表示的最大值，高位数会被扔掉，保留低位。当checksum=sum（data）时，数据正常，反之，则数据有损坏。但UDP不提供数据修复或数据重发机制，接收方知道数据损坏后，一般只是扔掉。而且UDP也无法得知数据包是否到达。
TCP-传输控制协议（Transmission Control Protocol）：如果“所有数据必须到达，就用TCP。”TCP的高级功能：1.TCP数据包有序号；2.TCP要求接收方的电脑收到数据包并且校验和检查无误后（数据无损坏），给发送方一个确认码，代表收到了。3.确认码的成功率和来回时间可以推测网络的拥堵程度。TCP用这个信息，调整同时发包数量，解决拥堵问题。
小结：TCP可以处理乱序和丢失数据包，还可以根据拥挤情况自动调整传输率。缺点：TCP确认码数据包把数量翻了一倍，但并没有传输更多的信息。
域名系统（Domain Name System，DNS）：负责把域名和IP地址一一对应。
开放式系统互联通信参考模型（Open System Interconnection，OSI）：应用程序层（Application Layer）、表示层（Presentation Layer）、会话层（Session Layer）、传输层（Transport Layer）、网络层（Network Layer）、数据链路层（Data Link Layer）、物理层（Physical Layer）。

## 30.《万维网》
如果说，互联网是网络世界中传输数据的网，那么万维网就是跑在其上最频繁的程序。

## 31.《计算机安全》
计算机安全，是保护系统数据的保密性，完整性和可用性。
保密性：是只有有权限的人才能读取计算机的系统和数据；（盗刷银行卡）
完整性：是只有有权限的人才能使用和修改系统和数据；（盗邮箱密码，假冒你发邮件）
可用性：是有权限的人应该随时可以访问系统和数据；（拒绝服务攻击DDOS：黑客发大量请求到服务器，让网站很慢或者挂掉）
威胁模型分析（threat model）：为了实现这三个目标，安全专家会从抽象层面想象“敌人”可能是谁。模型会对攻击者有大致描述：能力如何，目标可能是什么，可能用什么手段。
攻击手段又叫“攻击矢量（attack vector）”。威胁分析模型能为特定类型的手段做准备，不被可能的攻击手段数量所淹没。
身份验证的三种方式：你知道什么？你有什么？你是什么？
访问控制：通过“权限”或“访问控制列表（ACL）”来实现。权限类型：读、写、执行... 访问级别：公开、机密、顶级机密...
Bell-LaPadula模型：“不能向上读，不能向下写。”
隔离（isolation）：使用“沙盒”程序。操作系统会把程序放到沙盒里，方法是给每个程序独有的内存块，其他程序不能动。一台计算机可以运行多个虚拟机，每个虚拟机都在自己的沙箱里，如果一个程序出错，则最坏的情况是这个程序崩溃，或搞坏这个虚拟机的环境，其他的虚拟机和程序不受影响。

## 32.《黑客与攻击》
社会工程学：Social Engineering），是一种通过人际交流的方式获得信息的非技术渗透手段。不幸的是，这种手段非常有效，而且应用效率极高。事实上，社会工程学已是企业安全最大的威胁之一。常用社工手段：钓鱼、假托...
NAND镜像：如果能物理接触到电脑，可以往内存里接上几根线，复制整个内存；然后暴力解码，直到设备让你等待，这时只要把复制的内容覆盖掉内存，本质上重置了内存，就不用等待，可以继续暴力解码了。
漏洞利用（Exploit）：远程攻击者利用系统漏洞来获得某些能力或访问权限。
缓冲区溢出（buffer overflow）：会覆盖到其他重要数据，导致系统崩坏；或修改内存，绕过‘登录’之类的验证；或劫持系统。应对方法：边界检查；设置‘金丝雀’空间，检验数据变动情况；
代码注入：输入用户名时，输入两条指令，例如“'whatever'; DROP TABLE users”，这样可以删除整个用户密码表。
零日漏洞：没有被软件开发者发现的新漏洞。
蠕虫（Worms）：足够多的漏洞让恶意程序可以在电脑间互相传播；
僵尸网络（Botnet）：黑客拿下大量电脑，这些电脑可以组成‘僵尸网络’。

## 33.《加密》
因为有时黑帽黑客的攻击手段不止一种，为了加强防护，白帽黑客们采用了多层防御（defence in depth），来抵御进攻。
替换加密：算法把每个字母替换成其他字母。缺点：每个字母出现的频率是一样的。
凯撒加密：把信件中的每一个字母移位3个，即A->E。
移位加密：
列移位加密：把信息放入一个5*5或者其他矩阵表格，横放竖读，竖放横读。如果没有5*5的表格细则，则不能解密。
德国二战时创造了Enigma加密机。
密码学领域三大重要部分：对称加密、密钥交换，公钥密码学
其中密钥加密使用了“单向函数”原理，即采用Diffie-Hellman Key Exchange算法，对基底数求幂求模（base^x）mod(...)
非对称加密：公钥和私钥并存，公钥加密数据，只有私钥能解密数据。公钥不能解密私钥数据，因为不对称；反过来，可以用私钥加密，公钥解密。这种做法用于‘签名’，服务器用私钥加密，任何用户都可以用服务器的公钥解密。

## 34.《机器学习与人工智能》
机器学习是实现人工智能的众多技术之一。
机器学习根据已标签数据（labeled data）和其特征来制作分类器，从而对未标签数据（unlabeled data）进行分类，有时会产生一条决策边界来作为分类的依据。如果不能画出绝对的决策线，可以创造混淆矩阵，即最大限度地提高正确的分类数和降低错误的分类数。
常见的机器学习算法：决策树、支持向量机、人工神经网络、深度学习、强化学习。

## 35.《计算机视觉》
计算机用检测垂直边缘的算法来区分物体，把图片调成灰度，边界数值的差异越大，此处是边缘的可能性越大。
核/过滤器（kernel or filter）：一组矩阵值，不同的值组成的过滤器矩阵可以检测出不同的事物。
卷积神经网络：将核应用到图片的像素中，再运用类似神经元的计算方法，神经元有很多层，中间的每一层可以用一个核作权重处理，来提高识别度。

## 36.《自然语言处理》
语音识别技术是将人类语音接受为波形，再转化为谱图，这种技术是“傅里叶转换”。不同的字符具有不同的谱图分布规律，据此数据可以判断是什么字符。
通过语音识别，将所搜集到的自然语言声音转为文本，对文本进行词性分析和短语结构规划分析，最终得出结果。
构成单词的声音片段，叫“音素”。语音识别本质上是音素识别，英语大概有44种音素，例如“aa”、“eee”etc。
因为口音问题和日常口误，识别的文本有时会有明显的错误。结合语言模型后，转换准确率会大大提高。

## 37.《机器人》
机器人是由很多控制回路来控制的。
一个简单的控制回路包括：传感器、控制器（接受计算误差）和系统（待执行操作的指令和程序）。【用的“实际值”】
负反馈回路：传回误差给控制器，由控制器做出改变以减小误差。
比例-积分-微分控制器（PID 控制器）：
比例值：“实际值”与“理想值”差多少；实际值可能有一定滞后，或者是实时的。当两者差值越大，就越用力，因此它是比例控制的。
积分值：即一段时间内，误差的总和。当最近几秒误差值很大时，说明此时比例控制不够，要用力前进。
导数值：是期望值与实际值之间的变化率。有助于解决未来可能出现的错误，有时也叫“预期控制”。比如前进太快，要放松一点，避免冲过头。
这三个值会一起使用，并有不同的权重，然后用来控制系统。